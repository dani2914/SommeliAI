{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sanjeevtewani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pres import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Why do we care?\n",
    " We're interested in exploring probabilistic programming as it\n",
    " applies to LDA and its variants.  LDA describes an extremely\n",
    " intuitive generative process, and because of this it enjoys\n",
    " continued research into its expansions. It is also both flexible\n",
    " and interpretable.  Since everybody here knows what LDA is, we\n",
    " won't go over the details.\n",
    "\n",
    " But, inference is really, really hard.  Most expansions to LDA that\n",
    " get published require subtle tricks to even get inference working.\n",
    "\n",
    " Examples are:\n",
    "   1. Supervised LDA for classification only works for real-valued supervision;\n",
    "      (Blei & McAuliffe, 2008)\n",
    "   2. Multiple Classification LDA was published a full year later; it required a\n",
    "      subtle application of Jensen's inequality to reduce O(K^N) time to O(K^2)\n",
    "      (Wang et al., 2009)\n",
    "   3. Hierarchical Supervised LDA (Perotte, 2011) can't model a true \n",
    "      is-a-this-and-not-that hierarchical relationship\n",
    " Just about every incremental idea requires some special trick, no matter how\n",
    " logical the idea is.\n",
    "\n",
    "__In spite of how understandable and flexible LDA__ is, even statisticians and\n",
    "practitioners will have a tough time deploying criticizable models for their needs.\n",
    "\n",
    "Fortunately, using Pyro to overcome intractable integrals was day 2; so let's get started\n",
    "with LDA in Pyro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset and Course of Research\n",
    " We study almost 300,000 wine reviews from WineEnthusiast.com.  This dataset is\n",
    " richly tagged, with numerical scores from 0 to 100, hierarchical region information\n",
    " (country->region->winery) and variety.\n",
    " Reviews are short but basically already bags of words; we don't anticipate many words\n",
    " being wasted on nuanced semantics or stop words, so we believe that these reviews may\n",
    " be long enough for LDA to work.\n",
    " \n",
    " > \"Damp earth, black plum and dank forest herbs show\n",
    " > on the nose of this single-vineyard expression. The palate offers\n",
    " > cranberry and raspberry as well as savory soy and dried beef flavors,\n",
    " > all with earthy herbs in the background.\"\n",
    "\n",
    "As such, we expect to see a progression of topic modelling capabilities as we march\n",
    "down our list of models:\n",
    " - LDA,\n",
    " - LDA + classification v. supervised LDA,\n",
    " - Hierarchical LDA + classification v. supervised LDA\n",
    " - Hierarchical supervised LDA v. supervised LDA\n",
    " - Spectral Methods for supervised LDA v. supervised LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla LDA\n",
    "__Top Words Per Topic (10 topics)__\n",
    "![](files/words_per_topic_lda.png)\n",
    "\n",
    "There is great overlap of top words in each topic. That means our inference doesn't work. Perhaps our data is truly just a very poor fit for LDA; this was always a risk. So we tried to come up with a customised stop word list to remove words that show up in almost every review, like wine, tannin, and fruit. Then, we did different parameter initialization to tune the hyperparameters. We found random initialization to document dirichlet parameter and uniform initialization to topic dirichlet parameter yields better result. \n",
    "\n",
    "__LDA graphical model & mean field inference model__\n",
    "![](files/lda_plate_notation.png)\n",
    "To tweak the $model$ and $guide$, we compared the result for two different implementations of guide. \n",
    "1. Implement the exact same procedure as the mean field variational inference as shown in the figure above. \n",
    "2. Enumerate out the topic assignment discrete random variable $z$.\n",
    "\n",
    "But, the same topic words overlap still exists.\n",
    "\n",
    "\n",
    "# Supervised LDA?\n",
    "\n",
    "If the objective of LDA may itself not offer enough rewards to separate out topics, an extra loss may help; we can use sLDA to potentially learn more latent structure.\n",
    "\n",
    "Supervised LDA (sLDA) is a variant of LDA by adding an additional gaussian regression on topic assignment latent variable z and using the average topic assignments of words in a review to predict the score of the review.\n",
    "\n",
    "__sLDA graphical model__\n",
    "![](files/sLDA_graphical_model.png)\n",
    "Thus, in inference, to estimate the $\\eta$ and $\\sigma$, we put normal prior on them and let Pyro to derive an approximate posterior.\n",
    "\n",
    "\n",
    "# sLDA's top words per topic\n",
    "![](files/words_per_topic_slda.png)\n",
    "\n",
    "We still see word overlap showing up. At this point, we have an inkling that our data simply won't permit factorizing via LDA. But, convinced by the richness of the dataset and the natural topic-variety mapping in our heads, we remain undeterred.  Rather, we conclude that we can check our work by fitting the same topic model via CAVI, using code widely available on the internet (from blei-lab).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL VISUALIZATIONS\n",
    "\n",
    "## Data distribution\n",
    "\n",
    "__peek response variable distribution(scaled)__\n",
    "![](files/score_historgram.png)\n",
    "After scaling \n",
    "\n",
    "## LDA result\n",
    "\n",
    "## sLDA result\n",
    "\n",
    "### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_notebook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-832c3217467b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_pyro_lda_1_theta_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/x/coursework/coms6998/SommeliAI/notebooks/pres.py\u001b[0m in \u001b[0;36mplot_pyro_lda_1_theta_tsne\u001b[0;34m(refresh)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_pyro_lda_1_theta_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtheta_tsne_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pyro_lda_1_theta_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_tsne_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/x/coursework/coms6998/SommeliAI/notebooks/util.py\u001b[0m in \u001b[0;36mgraph_tsne\u001b[0;34m(tsne_df)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Plot the Topic Clusters using Bokeh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0moutput_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mgroup_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"group\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_notebook' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pyro_lda_1_theta_tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"3.1.py\", line 2, in <module>\r\n",
      "    import notebooks.util as util\r\n",
      "ModuleNotFoundError: No module named 'notebooks'\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Collapse\n",
    "\n",
    "We suspect that what we're experiencing is a collapse in all of our estimated distributions to the same point.  This could be MAP or another local optimum, and the literature seems vaguely aware that this can happen for topic models:\n",
    "\n",
    ">\"...despite some notable successes ...,black box inference methods are significantly more challenging to apply to topic models. For example, in initial experiments, we tried to apply ADVI (Kucukelbir et al., 2016), ..., but it was difficult to obtain any meaningful topics. Two main challenges are: first, the Dirichlet prior ... hinders reparameterisation, and second, the well known problem of component collapsing (Dinh & Dumoulin, 2016), in which the inference network becomes stuck in a bad local optimum in which all topics are identical\" (Srivastava and Sutton, 2017)\n",
    "\n",
    "However, most references to the phenomenon are in relation to VAEs, where it is also referred to as KL or mode collapse.  The cause is not well understood (Lucas et al., 2019), but the accepted definition is that it occurs when the distribution of latent variable $z$ given variational parameter $\\theta$ is fit such that $q(z \\vert \\theta) = p(z)$; samples of the latent variable become independent of the variational parameters.\n",
    "\n",
    "We hope that by scrutinizing each parameter update from Pyro and CAVI, we can get a sense of what's going on.  We can either trace parameters in Pyro (which is difficult to do because of how many parameters there are for LDA), or we can hand-derive the parameter updates it uses.  Pyro employs BBVI for discrete distributions; it will furthermore use context clues from plates to Rao-Blackwellize where possible, so for the remainder of this discussion we will work with the analytical updates we believe Pyro is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Recall the parameter updates from CAVI:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{d,n}^k &\\propto [E[\\log \\theta_{dk}] - E[\\log \\beta_{k,w_{d,n}}]] \\\\\n",
    "\\gamma_d &= \\alpha + \\sum_{n=1}^N \\phi_{dn}^k \\\\\n",
    "\\lambda_k &= \\eta + \\sum_{d=1}^D \\sum_{n=1}^N \\phi_{dn}^k w_{dn}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Versus the same updates for BBVI:\n",
    "\\begin{align}\n",
    "&\\gamma \\leftarrow \\gamma + \\rho\\nabla_\\gamma L(\\gamma); \\qquad \\phi_{d,n} \\leftarrow \\phi_{d,n} + \\rho\\nabla_{\\phi_{d,n}} L; \\\\\n",
    "&\\nabla_\\gamma L = E_{q(\\theta_d \\vert \\gamma)}\\left[\\nabla_\\gamma \\log q(\\theta_d \\vert \\gamma) \\left( \\log p (\\theta_d \\vert \\alpha) - \\log q(\\theta_d \\vert \\gamma) + E_{q(z_d \\vert \\phi_d)}[\\log p(z_d \\vert \\theta_d)] \\right) \\right] \\\\\n",
    "&\\nabla_{\\phi_{d,n}} L \\\\\n",
    "&\\:\\:\\:= E_{q(z_{d,n} \\vert \\phi_{d,n})}[\\nabla_{\\phi_{d,n}} \\log q(z_{d,n} \\vert \\phi_{d,n}) \\\\\n",
    "&\\qquad\\times \\left( \\log p (w_{d,n} \\vert z_{d,n}, \\beta) - \\log q (z_{d,n} \\vert \\phi_{d,n}) + E_{q(\\theta_d \\vert \\gamma)}[\\log p(z_{d,n} \\vert \\theta_d)] \\right) ] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "In practice, we estimate these expectations Via Monte-Carlo, which we can write down (over $B$ samples)\n",
    "\\begin{align}\n",
    "\\nabla&_\\gamma L(\\gamma) \\approx \\frac{1}{B} \\sum_{d=1}^B \\left(\\log \\theta_d - \\Psi(\\gamma) + \\Psi\\left(\\sum_{i=1}^K \\gamma _i\\right)\\right) \\\\\n",
    "& \\qquad\\times \\left(\\log(Dir(\\theta_d \\vert \\alpha))-\\log(Dir(\\theta_d \\vert \\gamma)) + \\frac{1}{N_d} \\sum_{n=1}^{N_d} \\log \\theta_{d}^T \\bar{z}_{d,n} \\right)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla&_{\\phi_d,n} L(\\phi_{d, n}) \\approx \\frac{1}{N_d}\\sum_{n=1}^{N_d} \\frac{1}{\\phi_{d,n,z_{d,n}}} \\left(\\log \\beta_{z_{d,n},w_{d,n}} - \\log \\phi_{d,n,z_{d,n}} + \\frac{1}{B} \\sum_{d=1}^B \\log \\theta_d^T z_{d,n} \\right)\n",
    "\\end{align}\n",
    "\n",
    "A few things to note:\n",
    " - Updates aren't comparable; CAVI is run until convergence while BBVI will step iteratively (along with other parameters).  But the similarities are evident\n",
    " - All updates show some tradeoff between updating a parameter and shrinking toward its prior. $\\theta$ isn't the prior of $\\phi$, but they are closely related via the model.\n",
    " - Both BBVI updates have Monte-Carlo estimates that can be written as KL divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How so?\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\gamma L(\\gamma) = E_{q(\\theta_d \\vert \\gamma)}\\left[\\nabla_\\gamma \\log q(\\theta_d \\vert \\gamma) \\left( \\log p (\\theta_d \\vert \\alpha) - \\log q(\\theta_d \\vert \\gamma) + \\underbrace{E_{q(z_d \\vert \\phi_d)}[\\log p(z_d \\vert \\theta_d)]}_{-KL(q(z_d \\vert \\phi_d)||p(z_d \\vert \\theta_d))} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\phi_{d,n}} L &(\\phi_{d,n})\\\\\n",
    "& = E_{q(z_{d,n} \\vert \\phi_{d,n})}\\left[\\nabla_{\\phi_{d,n}} \\log q(z_{d,n} \\vert \\phi_{d,n}) \\left( \\log p (w_{d,n} \\vert z_{d,n}, \\beta) - \\log q (z_{d,n} \\vert \\phi_{d,n}) + \\underbrace{E_{q(\\theta_d \\vert \\gamma)}[\\log p(z_{d,n} \\vert \\theta_d)]}_{+\\log p(z_{d,n}) - KL(q(\\theta_d \\vert \\gamma)||p(\\theta_d \\vert z_{d, n})) + KL(q(\\theta_d \\vert \\gamma) || p(\\theta_d))} \\right) \\right] \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expanded the second expectation by writing $p(z_{d,n}\\vert\\theta_d) = p(\\theta_d\\vert z_{d,n}) p(z_{d,n}) / p(\\theta_d)$.  There are two things to note:\n",
    " - The second KL divergence in the gradient for $\\phi$ can be ignored, because $\\theta_d$ and $\\gamma$ aren't in the Markov blanket of $\\phi_{d,n}$\n",
    " - $\\log(p(z_{d,n}) / q(z_{d,n} \\vert \\phi_{d,n}))$ should trigger some alarm bells.  This term will guide our variational distribution towards the unconditional $p(z)$: the hallmark of posterior collapse.  If these variational parameters are guided towards a collapsed maximum, the iterative gradient updates will drag all model parameters towards the collapse.\n",
    " \n",
    "__We believe that the noisy gradient updates in BBVI__ force us towards posterior collapse.  In the Monte Carlo expressions for the expectations above, you can see that __unlike with CAVI, at each iteration other variational parameters will only update the current one through dot products and KL divergences__.  Importantly, if the projection of our parameter on the others is small, all of the linkages in the model break down, and we experience a collapse.  Potentially we can overcome this with larger subsamples (reducing noise), but even Rao-Blackwellized estimation is too noisy for the sheer number of parameters we have.\n",
    "\n",
    "Note that we cannot work with either of $p(z)$ or $p(\\theta\\vert z)$; at best we can posit that the tradeoff in the updates for $\\phi$ can lead to poor local optimums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "### Summary\n",
    "\n",
    "We explored LDA and its variant sLDA in probabilistic programming language and tried to do inference on real world wine review text data. To dive deeper into why Pyro is not working, we tested our data with hand written CAVI and analyzed the gradient procedures in BBVI. We concluded that BBVI fails because of the big variance in each update as well as the simultaneous update of topic-word distributions and document-topic distributions. \n",
    "\n",
    "Models variation: LDA (with different versions of pyro implementation), sLDA\n",
    "Inference techniques: Variational Inference (CAVI and BBVI)\n",
    "Implementation strategies: Pyro\n",
    "\n",
    "### Challenges\n",
    "\n",
    "1. Getting Pyro-implemented LDA inference to work. We tried different implementation tricks, stop words removal and hypyterparameter tuning.\n",
    "2. Analyzing why Pyro fails in this BBVI procedure. We hand derived BBVI for Pyro and observe step by step how parameters are updated.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Identifying the problem in BBVI informs our next steps.  Options are:\n",
    "\n",
    "1. Set appropriate subsample size at each step; in experiments, we see we need very large batch sizes, (whereas batch VI succeeds with very small batches) (no good)\n",
    "2. Consider models without $\\phi$.  Given that we don't need to worry about tractability, perhaps we can use a collapsed LDA model.  (no good)\n",
    "3. Try models that work with different priors, like logistc normals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CAVI for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # References\n",
    " (Blei & McAuliffe, 2008) https://papers.nips.cc/paper/3328-supervised-topic-models.pdf\n",
    " (Wang et al., 2009) http://vision.stanford.edu/pdf/WangBleiFei-Fei_CVPR2009.pdf\n",
    " (Schroeder, 2018) https://edoc.hu-berlin.de/bitstream/handle/18452/19516/thesis_schroeder_ken.pdf?sequence=3\n",
    " (Perotte et al., 2011) https://papers.nips.cc/paper/4313-hierarchically-supervised-latent-dirichlet-allocation\n",
    " (Thoutt, 2017) https://www.kaggle.com/zynicide/wine-reviews\n",
    " (Srivastava and Sutton, 2017) https://arxiv.org/pdf/1703.01488.pdf\n",
    " (Lucas et al., 2019) https://openreview.net/pdf?id=r1xaVLUYuE"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
